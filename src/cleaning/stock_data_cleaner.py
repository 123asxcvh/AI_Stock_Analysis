#!/usr/bin/env python
"""
ç²¾ç®€ç‰ˆå¢å¼ºæ•°æ®æ¸…æ´—æ¨¡å—
åªä¿ç•™ä¸»æµç¨‹å’Œå¿…è¦è¾…åŠ©æ–¹æ³•
"""

import warnings
from pathlib import Path
from typing import Union

import numpy as np
import pandas as pd


warnings.filterwarnings("ignore")

# å¯¼å…¥ç»Ÿä¸€è·¯å¾„ç®¡ç†
import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.append(str(project_root))
from config import config


class EnhancedDataCleaner:
    def __init__(self, data_root_dir: Union[str, Path] = None):
        # ä½¿ç”¨ç°ä»£è·¯å¾„ç®¡ç†
        if data_root_dir is None:
            data_root_dir = str(config.data_dir)

        self.data_root_dir = Path(data_root_dir)
        self.stocks_dir = config.get_stocks_dir()
        self.cleaned_dir = self.data_root_dir / "cleaned_stocks"

        # åŠ è½½åŸºæœ¬é…ç½®
        self._load_external_configs()

        # å®šä¹‰éœ€è¦æ¸…æ´—çš„æ–‡ä»¶ç±»å‹
        self.cleaning_config = {
            "balance_sheet.csv": {
                "description": "èµ„äº§è´Ÿå€ºè¡¨æ•°æ®",
                "required_columns": ["æ—¥æœŸ"],
            },
            "income_statement.csv": {
                "description": "åˆ©æ¶¦è¡¨æ•°æ®",
                "required_columns": ["æ—¥æœŸ"],
            },
            "cash_flow_statement.csv": {
                "description": "ç°é‡‘æµé‡è¡¨æ•°æ®",
                "required_columns": ["æ—¥æœŸ"],
            },
            "main_business_composition.csv": {
                "description": "ä¸»è¥æ„æˆæ•°æ®",
                "required_columns": ["æ—¥æœŸ", "ä¸»è¥æ”¶å…¥", "ä¸»è¥æˆæœ¬"],
            },
            "financial_indicators.csv": {
                "description": "è´¢åŠ¡æŒ‡æ ‡æ•°æ®",
                "required_columns": ["æ—¥æœŸ", "å‡€èµ„äº§æ”¶ç›Šç‡", "èµ„äº§è´Ÿå€ºç‡"],
            },
              "stock_belong_boards.csv": {
                "description": "è‚¡ç¥¨æ‰€å±ç‰ˆå—æ•°æ®",
                "required_columns": ["è‚¡ç¥¨åç§°", "è‚¡ç¥¨ä»£ç ", "æ¿å—ä»£ç ", "æ¿å—åç§°"],
            },
            "intraday_data.csv": {
                "description": "åˆ†æ—¶æ•°æ®",
                "required_columns": ["æ—¥æœŸ", "å¼€ç›˜", "æ”¶ç›˜", "æœ€é«˜", "æœ€ä½"],
            },
            "company_profile.csv": {
                "description": "å…¬å¸æ¦‚å†µæ•°æ®",
                "required_columns": ["å­—æ®µå", "å­—æ®µå€¼"],
            },
            "bid_ask.csv": {
                "description": "ç›˜å£æ•°æ®",
                "required_columns": ["å­—æ®µå", "å­—æ®µå€¼"],
            },
            "peer_growth_comparison.csv": {
                "description": "åŒè¡Œæˆé•¿æ€§æ¯”è¾ƒæ•°æ®",
                "required_columns": ["ä»£ç ", "ç®€ç§°"],
            },
            "peer_valuation_comparison.csv": {
                "description": "åŒè¡Œä¼°å€¼æ¯”è¾ƒæ•°æ®",
                "required_columns": ["ä»£ç ", "ç®€ç§°"],
            },
            "peer_dupont_comparison.csv": {
                "description": "åŒè¡Œæœé‚¦åˆ†ææ¯”è¾ƒæ•°æ®",
                "required_columns": ["ä»£ç ", "ç®€ç§°"],
            },
            "peer_scale_comparison.csv": {
                "description": "åŒè¡Œå…¬å¸è§„æ¨¡æ¯”è¾ƒæ•°æ®",
                "required_columns": ["ä»£ç ", "ç®€ç§°"],
            },
        }
        self.cleaner_mapping = {
            "company_profile.csv": self._clean_company_profile,
            "stock_belong_boards.csv": self._clean_stock_belong_boards,
            "main_business_composition.csv": self._clean_main_business_composition,
            "intraday_data.csv": self._clean_intraday_data,
            "bid_ask.csv": self._clean_bid_ask,
            "balance_sheet.csv": self._clean_financial_data,
            "income_statement.csv": self._clean_financial_data,
            "cash_flow_statement.csv": self._clean_financial_data,
            "financial_indicators.csv": self._clean_financial_data,
            "peer_growth_comparison.csv": self._clean_peer_comparison_data,
            "peer_valuation_comparison.csv": self._clean_peer_comparison_data,
            "peer_dupont_comparison.csv": self._clean_peer_comparison_data,
            "peer_scale_comparison.csv": self._clean_peer_comparison_data,
        }
        self.date_column_mapping = {
            "æŠ¥å‘ŠæœŸ": "æ—¥æœŸ",
            "æ•°æ®æ—¥æœŸ": "æ—¥æœŸ",
            "äº¤æ˜“æ—¥æœŸ": "æ—¥æœŸ",
            "date": "æ—¥æœŸ",
            "Date": "æ—¥æœŸ",
        }

    def _load_external_configs(self):
        """åŠ è½½åŸºæœ¬é…ç½®"""
        self.filter_start_date = "2022-01-01"

    def clean_stock_data(self, symbol: str):
        """
        æ¸…æ´—è‚¡ç¥¨æ•°æ®

        Args:
            symbol: è‚¡ç¥¨ä»£ç 
        """
        print(f"ğŸ§¹ å¼€å§‹æ•°æ®æ¸…æ´—è‚¡ç¥¨ {symbol} çš„æ•°æ®...")

        stock_dir = self.stocks_dir / symbol
        cleaned_stock_dir = self.cleaned_dir / symbol

        if not stock_dir.exists():
            print(f"âŒ è‚¡ç¥¨æ•°æ®ç›®å½•ä¸å­˜åœ¨: {stock_dir}")
            return False

        cleaned_stock_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºæ¸…æ´—ç›®å½•: {cleaned_stock_dir}")

        # æ­£å¸¸æ¸…æ´—æ¨¡å¼ï¼šä»åŸå§‹æ•°æ®æ¸…æ´—åˆ°ç›®æ ‡ç›®å½•
        csv_files = [f.name for f in stock_dir.glob("*.csv")]
        print(f"ğŸ“Š å‘ç° {len(csv_files)} ä¸ªCSVæ–‡ä»¶éœ€è¦æ¸…æ´—")

        success_count = 0
        for i, file_name in enumerate(csv_files, 1):
            print(f"   ğŸ”„ [{i}/{len(csv_files)}] æ­£åœ¨æ¸…æ´—: {file_name}")
            raw_file_path = stock_dir / file_name
            cleaned_file_path = cleaned_stock_dir / file_name

            self._clean_file(raw_file_path, cleaned_file_path, file_name)
            success_count += 1

        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {success_count}/{len(csv_files)} ä¸ªæ–‡ä»¶æˆåŠŸ")

        # æ­¥éª¤ï¼šå¯¹é™¤äº†historical_quotes.csvä¹‹å¤–çš„æ‰€æœ‰æ–‡ä»¶è¿›è¡Œå€’åºæ’åˆ—
        print(f"ğŸ”„ å¯¹éhistorical_quotesæ–‡ä»¶è¿›è¡Œå€’åºå¤„ç†...")
        files_to_descend = [f for f in csv_files if f != "historical_quotes.csv"]
        descend_success = 0

        for file_name in files_to_descend:
            file_path = cleaned_stock_dir / file_name
            if file_path.exists():
                if self._sort_file_descending(file_path):
                    descend_success += 1
                    print(f"   âœ… {file_name} å€’åºå®Œæˆ")
                else:
                    print(f"   âš ï¸ {file_name} å€’åºå¤±è´¥")

        print(f"âœ… å€’åºå¤„ç†å®Œæˆ: {descend_success}/{len(files_to_descend)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        return success_count == len(csv_files)

    def clean_stock_data_skip_historical_quotes(self, symbol: str):
        """
        æ¸…æ´—è‚¡ç¥¨æ•°æ®ï¼Œä½†è·³è¿‡historical_quotes.csvæ–‡ä»¶ï¼ˆé¿å…ä¸æŠ€æœ¯æŒ‡æ ‡å‡†å¤‡å†²çªï¼‰
        Args:
            symbol: è‚¡ç¥¨ä»£ç 
        """
        print(f"ğŸ§¹ å¼€å§‹æ¸…æ´—è‚¡ç¥¨ {symbol} çš„æ•°æ®ï¼ˆè·³è¿‡è¡Œæƒ…æ•°æ®ï¼‰...")
        stock_dir = self.stocks_dir / symbol
        cleaned_stock_dir = self.cleaned_dir / symbol

        if not stock_dir.exists():
            print(f"âŒ è‚¡ç¥¨æ•°æ®ç›®å½•ä¸å­˜åœ¨: {stock_dir}")
            return False

        cleaned_stock_dir.mkdir(parents=True, exist_ok=True)
        print(f"ğŸ“ åˆ›å»ºæ¸…æ´—ç›®å½•: {cleaned_stock_dir}")

        csv_files = [f.name for f in stock_dir.glob("*.csv")]
        # è·³è¿‡historical_quotes.csvæ–‡ä»¶
        csv_files = [f for f in csv_files if f != "historical_quotes.csv"]

        print(f"ğŸ“Š å‘ç° {len(csv_files)} ä¸ªCSVæ–‡ä»¶éœ€è¦æ¸…æ´—ï¼ˆå·²è·³è¿‡historical_quotes.csvï¼‰")
        success_count = 0

        for i, file_name in enumerate(csv_files, 1):
            print(f"   ğŸ”„ [{i}/{len(csv_files)}] æ­£åœ¨æ¸…æ´—: {file_name}")
            raw_file_path = stock_dir / file_name
            cleaned_file_path = cleaned_stock_dir / file_name

            self._clean_file(raw_file_path, cleaned_file_path, file_name)
            success_count += 1

        print(f"âœ… æ•°æ®æ¸…æ´—å®Œæˆ: {success_count}/{len(csv_files)} ä¸ªæ–‡ä»¶æˆåŠŸ")
        return success_count == len(csv_files)

    def _clean_file(self, raw_file_path, cleaned_file_path, file_name):
        """è°ƒåº¦æ–‡ä»¶åˆ°å¯¹åº”çš„æ¸…æ´—å‡½æ•°"""
        df = pd.read_csv(raw_file_path)
        if df.empty:
            print(f"   âš ï¸ {file_name} ä¸ºç©ºæ–‡ä»¶ï¼Œè·³è¿‡æ¸…æ´—ã€‚")
            return

        # æ ¹æ®æ–‡ä»¶åè·å–å¯¹åº”çš„æ¸…æ´—å‡½æ•°ï¼Œè‹¥æ— ç‰¹å®šå‡½æ•°åˆ™ä½¿ç”¨é€šç”¨å‡½æ•°
        clean_function = self.cleaner_mapping.get(
            file_name, self._clean_generic_file
        )

        # ä¼ é€’æ–‡ä»¶åç»™éƒ¨åˆ†éœ€è¦å®ƒçš„å‡½æ•°
        if clean_function in [self._clean_financial_data, self._clean_generic_file, self._clean_peer_comparison_data]:
            cleaned_df = clean_function(df, file_name)
        else:
            cleaned_df = clean_function(df)

        if cleaned_df is not None and not cleaned_df.empty:
            cleaned_df.to_csv(cleaned_file_path, index=False, encoding="utf-8-sig")
            print(f"   âœ… {file_name} æ¸…æ´—å®Œæˆ: {len(cleaned_df)} æ¡è®°å½•")
        else:
            print(f"   â„¹ï¸ {file_name} æ¸…æ´—åæ— æ•°æ®ï¼Œä¸ä¿å­˜ã€‚")



    def _clean_generic_file(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """é€šç”¨æ•°æ®æ¸…æ´—æµç¨‹"""
        # å†å²æŠ¥ä»·æ•°æ®çš„åˆ—åæ˜ å°„
        historical_quotes_mapping = {
            "open": "å¼€ç›˜",
            "close": "æ”¶ç›˜",
            "high": "æœ€é«˜",
            "low": "æœ€ä½",
            "amount": "æˆäº¤é‡"
        }

        # è´¢åŠ¡æŒ‡æ ‡çš„åˆ—åæ˜ å°„
        financial_indicator_mapping = {
            "å‡€èµ„äº§æ”¶ç›Šç‡(%)": "å‡€èµ„äº§æ”¶ç›Šç‡",
            "èµ„äº§è´Ÿå€ºç‡(%)": "èµ„äº§è´Ÿå€ºç‡",
            "PE(TTM)": "å¸‚ç›ˆç‡",
            "PE(é™)": "å¸‚ç›ˆç‡(é™)",
            "å¸‚å‡€ç‡(PB)": "å¸‚å‡€ç‡",
        }

        # åº”ç”¨åˆ—åæ˜ å°„
        if file_name == "historical_quotes.csv":
            df = df.rename(columns=historical_quotes_mapping)
        elif (
            file_name == "financial_indicators.csv"
            or file_name == "stock_valuation.csv"
        ):
            df = df.rename(columns=financial_indicator_mapping)

        df = self._standardize_date_columns(df)
        df = df.dropna(how="all").drop_duplicates()
        df = self._remove_stock_code_column(df)

        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
            df = df.dropna(subset=["æ—¥æœŸ"])
            df = self._filter_by_date(df, "æ—¥æœŸ", file_name)
            df = df.sort_values("æ—¥æœŸ", ascending=True)


        df = self._move_date_column_to_first(df)
        df = df.reset_index(drop=True)
        return df

    def _clean_stock_belong_boards(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—è‚¡ç¥¨æ‰€å±ç‰ˆå—æ•°æ®"""

        # åˆ é™¤ç¬¬ä¸€åˆ—ï¼ˆç´¢å¼•åˆ—ï¼‰
        if df.columns[0] == df.index.name or df.columns[0].startswith('Unnamed'):
            df = df.iloc[:, 1:]  # åˆ é™¤ç¬¬ä¸€åˆ—

        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        required_cols = ["è‚¡ç¥¨åç§°", "è‚¡ç¥¨ä»£ç ", "æ¿å—ä»£ç ", "æ¿å—åç§°"]
        available_cols = [col for col in required_cols if col in df.columns]

        if not available_cols:
            return pd.DataFrame()

        # é€‰æ‹©éœ€è¦çš„åˆ—
        df = df[available_cols + [col for col in ["æ¿å—æ¶¨å¹…"] if col in df.columns]]

        # åˆ é™¤é‡å¤è¡Œ
        df = df.drop_duplicates()

        # æŒ‰æ¿å—æ¶¨å¹…æ’åºï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if "æ¿å—æ¶¨å¹…" in df.columns:
            # å°†æ¶¨å¹…è½¬æ¢ä¸ºæ•°å€¼ç±»å‹
            df["æ¿å—æ¶¨å¹…"] = pd.to_numeric(df["æ¿å—æ¶¨å¹…"], errors="coerce")
            # æ’åºï¼Œå°†NaNå€¼æ”¾åœ¨æœ€å
            df = df.sort_values("æ¿å—æ¶¨å¹…", ascending=False, na_position='last')

        df = df.reset_index(drop=True)
        return df

  
    def _clean_main_business_composition(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ä¸»è¥æ„æˆæ•°æ®"""
        # 1. åˆ—é‡å‘½å
        rename_dict = {
            "æŠ¥å‘Šæ—¥æœŸ": "æ—¥æœŸ",
            "ä¸»è¥ä¸šåŠ¡æ”¶å…¥": "ä¸»è¥æ”¶å…¥",
            "ä¸»è¥ä¸šåŠ¡æˆæœ¬": "ä¸»è¥æˆæœ¬",
        }
        df = df.rename(columns=rename_dict)
        
        # 2. åˆ é™¤è‚¡ç¥¨ä»£ç åˆ—
        if "è‚¡ç¥¨ä»£ç " in df.columns:
            df = df.drop(columns=["è‚¡ç¥¨ä»£ç "])
        
        # 3. å¤„ç†åˆ†ç±»ç±»å‹
        if "åˆ†ç±»ç±»å‹" in df.columns:
            df["åˆ†ç±»ç±»å‹"] = df["åˆ†ç±»ç±»å‹"].fillna("æŒ‰è¡Œä¸šåˆ†ç±»")
            df["åˆ†ç±»ç±»å‹"] = df["åˆ†ç±»ç±»å‹"].replace("", "æŒ‰è¡Œä¸šåˆ†ç±»")

        # 4. å¤„ç†ä¸»è¥æ„æˆä¸­çš„"å…¶ä»–"é¡¹
        if "ä¸»è¥æ„æˆ" in df.columns:
            # å°†åŒ…å«"å…¶ä»–"çš„ä¸»è¥æ„æˆç»Ÿä¸€æ”¹ä¸º"å…¶ä»–"
            df["ä¸»è¥æ„æˆ"] = df["ä¸»è¥æ„æˆ"].apply(
                lambda x: "å…¶ä»–" if pd.notna(x) and "å…¶ä»–" in str(x) else x
            )

        # 5. ç§»åŠ¨æ—¥æœŸåˆ—åˆ°ç¬¬ä¸€åˆ—
        df = self._move_date_column_to_first(df)
        
        # 5. æ¸…ç†æ•°æ®
        df = df.dropna(how="all").drop_duplicates()

        # 6. å¤„ç†æ—¥æœŸåˆ—
        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
            df = df.dropna(subset=["æ—¥æœŸ"])
            # ä¸»è¥ä¸šåŠ¡æ„æˆæ•°æ®ä»2022å¹´å¼€å§‹
            df = self._filter_by_date(df, "æ—¥æœŸ", "main_business_composition.csv")
            df = df.sort_values("æ—¥æœŸ", ascending=True)

        df = df.reset_index(drop=True)
        return df

    def _clean_intraday_data(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—åˆ†æ—¶æ•°æ® - å‚è€ƒbaostockæ¥å£æ ¼å¼"""
        # åˆ é™¤ä¸éœ€è¦çš„åˆ—ï¼šdate, code, adjustflag
        columns_to_drop = ['date', 'code', 'adjustflag']
        df = df.drop(columns=[col for col in columns_to_drop if col in df.columns])

        # é‡å‘½ååˆ—ä¸ºä¸­æ–‡
        column_mapping = {
            'time': 'æ—¥æœŸ',
            'open': 'å¼€ç›˜',
            'high': 'æœ€é«˜',
            'low': 'æœ€ä½',
            'close': 'æ”¶ç›˜',
            'volume': 'æˆäº¤é‡',
            'amount': 'æˆäº¤é¢'
        }
        df = df.rename(columns=column_mapping)

        # å¤„ç†æ—¶é—´æ ¼å¼ï¼šå°†timeæ ¼å¼è½¬æ¢ä¸ºæ—¥æœŸæ—¶é—´
        if "æ—¥æœŸ" in df.columns:
            # baostockçš„timeæ ¼å¼ä¸ºï¼š20251118144000000
            # è½¬æ¢ä¸ºæ ‡å‡†datetimeæ ¼å¼
            df["æ—¥æœŸ"] = df["æ—¥æœŸ"].astype(str)
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], format='%Y%m%d%H%M%S%f', errors='coerce')
            df = df.dropna(subset=["æ—¥æœŸ"])

        # åŸºæœ¬æ¸…æ´—
        df = df.dropna(how="all").drop_duplicates()

        # æ­£å¸¸æ¸…æ´—æ—¶å‡åºæ’åˆ—ï¼ˆä¸ºäº†æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼‰
        if "æ—¥æœŸ" in df.columns:
            df = df.sort_values("æ—¥æœŸ", ascending=True)

        # å°†æ—¥æœŸåˆ—ç§»åˆ°ç¬¬ä¸€åˆ—
        if "æ—¥æœŸ" in df.columns:
            cols = df.columns.tolist()
            cols.remove("æ—¥æœŸ")
            df = df[["æ—¥æœŸ"] + cols]

        df = df.reset_index(drop=True)
        return df

    def _clean_company_profile(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—å…¬å¸æ¦‚å†µæ•°æ® - è½¬æ¢ä¸ºçºµå‘æ’åˆ—"""
        # å¦‚æœå·²ç»æ˜¯çºµå‘æ ¼å¼ï¼ˆæœ‰å­—æ®µåå’Œå­—æ®µå€¼åˆ—ï¼‰ï¼Œç›´æ¥è¿”å›
        if "å­—æ®µå" in df.columns and "å­—æ®µå€¼" in df.columns:
            return df
        
        # å°†æ¨ªå‘æ•°æ®è½¬æ¢ä¸ºçºµå‘æ ¼å¼
        if len(df) == 0:
            return pd.DataFrame()
        
        # å–ç¬¬ä¸€è¡Œæ•°æ®ï¼ˆé€šå¸¸åªæœ‰ä¸€è¡Œï¼‰
        row_data = df.iloc[0]
        
        # åˆ›å»ºçºµå‘æ ¼å¼çš„DataFrame
        vertical_data = []
        for column_name, value in row_data.items():
            if pd.notna(value) and str(value).strip() != "":
                vertical_data.append({
                    "å­—æ®µå": column_name,
                    "å­—æ®µå€¼": str(value).strip()
                })
        
        vertical_data.append({
            "å­—æ®µå": "æ•°æ®æ¸…æ´—æ—¶é—´",
            "å­—æ®µå€¼": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
        result_df = pd.DataFrame(vertical_data)
        
        return result_df

    def _clean_financial_data(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """æ¸…æ´—è´¢åŠ¡æŠ¥è¡¨æ•°æ® (èµ„äº§è´Ÿå€ºè¡¨, åˆ©æ¶¦è¡¨, ç°é‡‘æµé‡è¡¨)"""
        # 1. åˆ é™¤å®Œå…¨ç©ºç™½çš„åˆ—
        df = self._remove_empty_columns(df)

        # 2. ç»Ÿä¸€å¤„ç†è´¢åŠ¡æŠ¥è¡¨
        if file_name in ["balance_sheet.csv", "income_statement.csv", "cash_flow_statement.csv"]:
            df = self._clean_financial_statements(df, file_name)

        # 3. æ¸…æ´—è´¢åŠ¡æ•°å€¼
        skip_columns = ["æ—¥æœŸ", "æŠ¥å‘ŠæœŸ", "æŠ¥è¡¨æ ¸å¿ƒæŒ‡æ ‡", "æŠ¥è¡¨å…¨éƒ¨æŒ‡æ ‡"]
        for col in df.columns:
            if col in skip_columns:
                continue
            df[col] = df[col].apply(self._clean_financial_value)

        if "æŠ¥å‘ŠæœŸ" in df.columns:
            df["æŠ¥å‘ŠæœŸ"] = df["æŠ¥å‘ŠæœŸ"].apply(self._convert_year_to_date)
            df = df.rename(columns={"æŠ¥å‘ŠæœŸ": "æ—¥æœŸ"})

        if "æ—¥æœŸ" in df.columns:
            df["æ—¥æœŸ"] = pd.to_datetime(df["æ—¥æœŸ"], errors="coerce")
            df = df.dropna(subset=["æ—¥æœŸ"])
            df = self._filter_by_date(df, "æ—¥æœŸ", file_name)
            df = df.sort_values("æ—¥æœŸ", ascending=True)

        df = self._move_date_column_to_first(df)
        df = df.reset_index(drop=True)
        return df

    def _clean_financial_statements(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """ç»Ÿä¸€çš„è´¢åŠ¡æŠ¥è¡¨æ¸…æ´—å‡½æ•°ï¼ŒæŒ‰ç”¨æˆ·è¦æ±‚ç²¾ç¡®æ¸…æ´—"""
        columns_to_drop = []

        # 1. é¦–å…ˆåˆ é™¤å…¨ç©ºçš„åˆ—ï¼ˆæ‰€æœ‰è´¢åŠ¡æŠ¥è¡¨é€šç”¨ï¼‰
        empty_columns = []
        for col in df.columns:
            if col not in ["æŠ¥å‘ŠæœŸ", "æ—¥æœŸ"] and df[col].isnull().all():
                empty_columns.append(col)

        if empty_columns:
            columns_to_drop.extend(empty_columns)
            print(f"   ğŸ—‘ï¸ åˆ é™¤ç©ºç™½åˆ—: {empty_columns}")

        # 2. æŒ‰æ–‡ä»¶ç±»å‹è¿›è¡Œç‰¹æ®Šå¤„ç†
        if file_name == "income_statement.csv":
            # åˆ©æ¶¦è¡¨ç‰¹æ®Šå¤„ç†ï¼šæŒ‰ç”¨æˆ·è¦æ±‚

            # 2.1 åˆ é™¤æ‰€æœ‰å¸¦*çš„åˆ—
            star_columns = [col for col in df.columns if col.startswith("*")]
            columns_to_drop.extend(star_columns)

            # 2.2 æŸ¥æ‰¾"äº”ã€å‡€åˆ©æ¶¦"åˆ—çš„ä½ç½®ï¼Œåˆ é™¤ä¹‹åçš„æ‰€æœ‰åˆ—
            columns = [col for col in df.columns if col not in columns_to_drop]
            net_profit_index = None

            for i, col in enumerate(columns):
                if col == "äº”ã€å‡€åˆ©æ¶¦":
                    net_profit_index = i
                    print(f"   ğŸ“ æ‰¾åˆ°äº”ã€å‡€åˆ©æ¶¦åˆ—åœ¨ç¬¬ {i} ä¸ªä½ç½®")
                    break

            if net_profit_index is not None:
                # åˆ é™¤äº”ã€å‡€åˆ©æ¶¦ä¹‹åçš„æ‰€æœ‰åˆ—
                columns_after_net_profit = columns[net_profit_index + 1:]
                columns_to_drop.extend(columns_after_net_profit)
                print(f"   ğŸ—‘ï¸ åˆ©æ¶¦è¡¨åˆ é™¤å‡€åˆ©æ¶¦ä¹‹ååˆ—: {len(columns_after_net_profit)} ä¸ª")

            # 2.3 é‡å‘½åæŠ¥å‘ŠæœŸä¸ºæ—¥æœŸ
            if "æŠ¥å‘ŠæœŸ" in df.columns:
                df = df.rename(columns={"æŠ¥å‘ŠæœŸ": "æ—¥æœŸ"})
                print(f"   ğŸ”„ é‡å‘½å'æŠ¥å‘ŠæœŸ'ä¸º'æ—¥æœŸ'")

            # æ˜¾ç¤ºåˆ é™¤çš„*åˆ—
            if star_columns:
                print(f"   ğŸ—‘ï¸ åˆ é™¤å¸¦*åˆ—: {len(star_columns)} ä¸ª")
                if len(star_columns) <= 8:
                    print(f"      åˆ é™¤çš„åˆ—: {star_columns}")

        elif file_name == "balance_sheet.csv":
            # èµ„äº§è´Ÿå€ºè¡¨ï¼šåˆ é™¤å¸¦*çš„åˆè®¡åˆ—å’Œå¸¦"å…¶ä¸­"çš„å­é¡¹åˆ—
            for col in df.columns:
                if col not in ["æŠ¥å‘ŠæœŸ", "æ—¥æœŸ"]:
                    if col.startswith("*") or "å…¶ä¸­" in col:
                        columns_to_drop.append(col)

        elif file_name == "cash_flow_statement.csv":
            # ç°é‡‘æµè¡¨ï¼šåˆ é™¤å¸¦*çš„åˆè®¡åˆ—å’Œå¸¦"å…¶ä¸­"çš„å­é¡¹åˆ—ï¼Œä»¥åŠå‡€åˆ©æ¶¦ä¹‹åçš„åˆ—
            for col in df.columns:
                if col not in ["æŠ¥å‘ŠæœŸ", "æ—¥æœŸ"]:
                    if col.startswith("*") or "å…¶ä¸­" in col:
                        columns_to_drop.append(col)

            # æŸ¥æ‰¾å‡€åˆ©æ¶¦ç›¸å…³åˆ—å¹¶åˆ é™¤ä¹‹ååˆ—
            columns = [col for col in df.columns if col not in columns_to_drop]
            net_profit_index = None

            for i, col in enumerate(columns):
                if "å‡€åˆ©æ¶¦" in col:
                    net_profit_index = i
                    break

            if net_profit_index is not None:
                columns_after_net_profit = columns[net_profit_index + 1:]
                columns_to_drop.extend(columns_after_net_profit)
                print(f"   ğŸ—‘ï¸ ç°é‡‘æµé‡è¡¨åˆ é™¤å‡€åˆ©æ¶¦åŠä¹‹ååˆ—: {len(columns_after_net_profit)} ä¸ª")

        # 3. æ‰§è¡Œåˆ—åˆ é™¤
        if columns_to_drop:
            file_display_name = {
                "balance_sheet.csv": "èµ„äº§è´Ÿå€ºè¡¨",
                "income_statement.csv": "åˆ©æ¶¦è¡¨",
                "cash_flow_statement.csv": "ç°é‡‘æµé‡è¡¨"
            }.get(file_name, file_name)

            remaining_cols = len(df.columns) - len(columns_to_drop)
            print(f"   ğŸ—‘ï¸ {file_display_name}åˆ é™¤ç‰¹æ®Šåˆ—: {len(columns_to_drop)} ä¸ª")
            print(f"      ä¿ç•™åˆ—æ•°: {remaining_cols}")

            # æ˜¾ç¤ºåˆ é™¤çš„åˆ—åï¼ˆå¦‚æœæ•°é‡ä¸å¤šï¼‰
            if len(columns_to_drop) <= 10:
                print(f"      åˆ é™¤çš„åˆ—: {columns_to_drop}")

            df = df.drop(columns=columns_to_drop)

        return df

  
    def _clean_balance_sheet(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—èµ„äº§è´Ÿå€ºè¡¨ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self._clean_financial_statements(df, "balance_sheet.csv")

    def _clean_income_statement(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—åˆ©æ¶¦è¡¨ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰"""
        return self._clean_financial_statements(df, "income_statement.csv")

    def _remove_empty_columns(self, df: pd.DataFrame) -> pd.DataFrame:
        """åˆ é™¤å®Œå…¨ç©ºç™½çš„åˆ—"""
        # ä¿ç•™æ—¥æœŸåˆ—ï¼Œå³ä½¿å®ƒæœ‰ç©ºå€¼
        date_columns = [col for col in df.columns if 'æ—¥æœŸ' in col or 'date' in col.lower()]

        # æ‰¾å‡ºå®Œå…¨ä¸ºç©ºçš„åˆ—ï¼ˆé™¤äº†æ—¥æœŸåˆ—ï¼‰
        empty_columns = []
        for col in df.columns:
            if col in date_columns:
                continue
            if df[col].isnull().all() or (df[col] == "").all():
                empty_columns.append(col)

        if empty_columns:
            print(f"   ğŸ—‘ï¸ åˆ é™¤ç©ºç™½åˆ—: {empty_columns}")
            df = df.drop(columns=empty_columns)

        return df

    def _clean_cash_flow_statement(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ç°é‡‘æµé‡è¡¨ï¼Œåˆ é™¤è¡¥å……èµ„æ–™åŠä¹‹åçš„åˆ—"""
        # æŸ¥æ‰¾å‡€åˆ©æ¶¦çš„ä½ç½®ï¼ˆå‡€åˆ©æ¶¦æ˜¯è¡¥å……èµ„æ–™çš„å¼€å§‹ï¼‰
        net_income_index = None
        for i, col in enumerate(df.columns):
            if col == "å‡€åˆ©æ¶¦":
                net_income_index = i
                break

        if net_income_index is not None:
            # ä¿ç•™å‡€åˆ©æ¶¦ä¹‹å‰çš„æ‰€æœ‰åˆ—
            columns_to_keep = df.columns[:net_income_index].tolist()
            columns_to_drop = df.columns[net_income_index:].tolist()

            print(f"   ğŸ—‘ï¸ ç°é‡‘æµé‡è¡¨åˆ é™¤è¡¥å……èµ„æ–™åˆ—: {len(columns_to_drop)} ä¸ª")
            print(f"      ä¿ç•™åˆ—æ•°: {len(columns_to_keep)}, åˆ é™¤åˆ—æ•°: {len(columns_to_drop)}")
            print(f"      ä» '{columns_to_drop[0]}' å¼€å§‹åˆ é™¤åˆ°æœ€å")

            df = df.drop(columns=columns_to_drop)

        return df

    def _clean_financial_value(self, value):
        if pd.isna(value) or value == "" or value is None:
            return np.nan
        str_value = str(value).strip()
        if str_value.lower() in ["false", "true", "--", "-", ""]:
            return np.nan
        import re

        multiplier = 1
        if "ä¸‡äº¿" in str_value:
            multiplier = 1e12
            str_value = str_value.replace("ä¸‡äº¿", "")
        elif "åƒäº¿" in str_value:
            multiplier = 1e11
            str_value = str_value.replace("åƒäº¿", "")
        elif "ç™¾äº¿" in str_value:
            multiplier = 1e10
            str_value = str_value.replace("ç™¾äº¿", "")
        elif "åäº¿" in str_value:
            multiplier = 1e9
            str_value = str_value.replace("åäº¿", "")
        elif "äº¿" in str_value:
            multiplier = 1e8
            str_value = str_value.replace("äº¿", "")
        elif "ä¸‡" in str_value:
            multiplier = 1e4
            str_value = str_value.replace("ä¸‡", "")
        str_value = re.sub(r"[^\d.-]", "", str_value)
        if str_value == "" or str_value == "-":
            return np.nan
        numeric_value = float(str_value) * multiplier
        return numeric_value

    def _standardize_date_columns(self, df):
        for col, std_col in self.date_column_mapping.items():
            if col in df.columns and std_col != col:
                df[std_col] = df[col]
                df = df.drop(columns=[col])
        return df


    def _convert_year_to_date(self, value):
        value = str(value)
        if len(value) == 4 and value.isdigit():
            return f"{value}-12-31"
        return value

    def _filter_by_date(self, df, date_col, file_name=None):
            df[date_col] = pd.to_datetime(df[date_col], errors="coerce")

            if file_name and "talib" in file_name:
                filter_date = pd.to_datetime("2020-01-01")
                print(f"   ğŸ“… {file_name} æ—¶é—´è¿‡æ»¤: ä¿ç•™ä» {filter_date.strftime('%Y-%m-%d')} å¼€å§‹çš„æ•°æ®ï¼ˆæŠ€æœ¯æŒ‡æ ‡éœ€è¦æ›´é•¿å†å²ï¼‰")
            elif file_name == "main_business_composition.csv":
                # ä¸»è¥ä¸šåŠ¡æ„æˆæ•°æ®ä»2022å¹´å¼€å§‹
                filter_date = pd.to_datetime("2022-01-01")
                print(f"   ğŸ“… {file_name} æ—¶é—´è¿‡æ»¤: ä¿ç•™ä» {filter_date.strftime('%Y-%m-%d')} å¼€å§‹çš„æ•°æ®")
            else:
                filter_date = pd.to_datetime(self.filter_start_date)
                print(f"   ğŸ“… {file_name} æ—¶é—´è¿‡æ»¤: ä¿ç•™ä» {filter_date.strftime('%Y-%m-%d')} å¼€å§‹çš„æ•°æ®")

            filtered_df = df[df[date_col] >= filter_date]
            return filtered_df

    def _remove_stock_code_column(self, df):
        stock_code_columns = ["è‚¡ç¥¨ä»£ç ", "stock_code", "code", "ä»£ç ", "è¯åˆ¸ä»£ç "]
        for col in stock_code_columns:
            if col in df.columns:
                sample_values = df[col].dropna().head(20).astype(str).tolist()
                if len(sample_values) == 0:
                    continue
                is_stock_code = True
                for value in sample_values:
                    if not (
                        value.isdigit()
                        and (len(value) == 6 or (len(value) == 1 and value == "1"))
                    ):
                        is_stock_code = False
                        break
                if is_stock_code:
                    df = df.drop(columns=[col])
                    break
        return df

    def _move_date_column_to_first(self, df):
        if "æ—¥æœŸ" in df.columns:
            cols = df.columns.tolist()
            cols.remove("æ—¥æœŸ")
            new_cols = ["æ—¥æœŸ"] + cols
            df = df[new_cols]
        return df


    def _clean_bid_ask(self, df: pd.DataFrame) -> pd.DataFrame:
        """æ¸…æ´—ç›˜å£è¡Œæƒ…æ•°æ® - è½¬æ¢ä¸ºçºµå‘æ’åˆ—"""
        # å¦‚æœå·²ç»æ˜¯çºµå‘æ ¼å¼ï¼ˆæœ‰å­—æ®µåå’Œå­—æ®µå€¼åˆ—ï¼‰ï¼Œç›´æ¥è¿”å›
        if "å­—æ®µå" in df.columns and "å­—æ®µå€¼" in df.columns:
            return df
        
        # å¦‚æœæ•°æ®æ˜¯item,valueæ ¼å¼ï¼Œè½¬æ¢ä¸ºå­—æ®µå,å­—æ®µå€¼æ ¼å¼
        if "item" in df.columns and "value" in df.columns:
            result_df = df.rename(columns={"item": "å­—æ®µå", "value": "å­—æ®µå€¼"})
            
            return result_df
        
        # å°†æ¨ªå‘æ•°æ®è½¬æ¢ä¸ºçºµå‘æ ¼å¼
        if len(df) == 0:
            return pd.DataFrame()
        
        # å–ç¬¬ä¸€è¡Œæ•°æ®ï¼ˆé€šå¸¸åªæœ‰ä¸€è¡Œï¼‰
        row_data = df.iloc[0]
        
        # åˆ›å»ºçºµå‘æ ¼å¼çš„DataFrame
        vertical_data = []
        for column_name, value in row_data.items():
            if pd.notna(value) and str(value).strip() != "":
                vertical_data.append({
                    "å­—æ®µå": column_name,
                    "å­—æ®µå€¼": str(value).strip()
                })
        
        vertical_data.append({
            "å­—æ®µå": "æ•°æ®æ¸…æ´—æ—¶é—´",
            "å­—æ®µå€¼": pd.Timestamp.now().strftime("%Y-%m-%d %H:%M:%S")
        })
        
        result_df = pd.DataFrame(vertical_data)
        
        return result_df



    def _clean_peer_comparison_data(self, df: pd.DataFrame, file_name: str) -> pd.DataFrame:
        """æ¸…æ´—åŒè¡Œæ¯”è¾ƒæ•°æ® - åŸºæœ¬æ¸…æ´—ï¼Œä¿æŒåŸæ ¼å¼"""
        # åŸºæœ¬æ¸…æ´—ï¼šç§»é™¤ç©ºè¡Œå’Œé‡å¤è¡Œ
        df = df.dropna(how="all").drop_duplicates()
        
        # ç¡®ä¿ä»£ç åˆ—æ˜¯å­—ç¬¦ä¸²æ ¼å¼
        if "ä»£ç " in df.columns:
            df["ä»£ç "] = df["ä»£ç "].astype(str)
        df["ä»£ç "] = df["ä»£ç "].apply(lambda x: str(x).zfill(6) if str(x).isdigit() and len(str(x)) < 6 else str(x))
        
        ranking_columns = [col for col in df.columns if "æ’å" in col]
        for col in ranking_columns:
            pass
        
        forecast_columns = [col for col in df.columns if any(suffix in col for suffix in ["25E", "26E", "27E"])]
        for col in forecast_columns:
            pass
        
        df = df.reset_index(drop=True)

        return df

    def _sort_file_descending(self, file_path: Path) -> bool:
        """
        å°†æ–‡ä»¶æŒ‰æ—¥æœŸå€’åºæ’åˆ—

        Args:
            file_path: æ–‡ä»¶è·¯å¾„

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            # ä½¿ç”¨utf-8-sigç¼–ç æ¥å¤„ç†BOMï¼Œå¹¶æ¸…ç†åˆ—å
            df = pd.read_csv(file_path, encoding='utf-8-sig')

            # æ¸…ç†åˆ—åä¸­çš„BOMå­—ç¬¦å’Œç©ºç™½å­—ç¬¦
            df.columns = df.columns.str.replace('\ufeff', '').str.strip()

            if 'æ—¥æœŸ' in df.columns:
                df['æ—¥æœŸ'] = pd.to_datetime(df['æ—¥æœŸ'], errors='coerce')

                df = df.sort_values('æ—¥æœŸ', ascending=False)
                # ä¿å­˜æ—¶ä¸ä½¿ç”¨BOMï¼Œä½¿ç”¨æ ‡å‡†utf-8ç¼–ç 
                df.to_csv(file_path, index=False, encoding='utf-8')
                return True
            else:
                print(f"   âš ï¸ {file_path.name} æœªæ‰¾åˆ°æ—¥æœŸåˆ—ï¼Œè·³è¿‡æ’åº")
                return False
        except Exception as e:
            print(f"   âŒ å¤„ç† {file_path.name} æ—¶å‡ºé”™: {e}")
            return False

    def sort_symbol_data_descending(self, symbol: str) -> bool:
        """
        å°†æŒ‡å®šè‚¡ç¥¨çš„æ‰€æœ‰æ•°æ®æ–‡ä»¶æŒ‰æ—¥æœŸé™åºæ’åˆ—ï¼ˆæœ€ç»ˆæ’åºæ­¥éª¤ï¼‰

        Args:
            symbol: è‚¡ç¥¨ä»£ç 

        Returns:
            bool: æ˜¯å¦æˆåŠŸ
        """
        try:
            cleaned_stock_dir = self.cleaned_dir / symbol
            if not cleaned_stock_dir.exists():
                print(f"   âš ï¸ ç›®å½•ä¸å­˜åœ¨: {cleaned_stock_dir}")
                return False

            print(f"   ğŸ”„ å¼€å§‹æœ€ç»ˆæ•°æ®é™åºæ’åº: {symbol}")

            # éœ€è¦æ’åºçš„ä¸»è¦æ–‡ä»¶
            files_to_sort = [
                "historical_quotes.csv",
                "income_statement.csv",
                "balance_sheet.csv",
                "cash_flow_statement.csv",
                "financial_indicators.csv",
                "stock_valuation.csv",
                "intraday_data.csv",
                "main_business_composition.csv",
                  "stock_belong_boards.csv"
            ]

            success_count = 0
            total_files = 0

            for filename in files_to_sort:
                file_path = cleaned_stock_dir / filename
                if file_path.exists():
                    total_files += 1
                    if self._sort_file_descending(file_path):
                        success_count += 1

            print(f"   âœ… æœ€ç»ˆæ•°æ®æ’åºå®Œæˆ: {success_count}/{total_files} ä¸ªæ–‡ä»¶æˆåŠŸ")
            return total_files == 0 or success_count > 0

        except Exception as e:
            print(f"   âŒ æœ€ç»ˆæ’åºæ—¶å‡ºé”™: {e}")
            return False



if __name__ == "__main__":
    import argparse
    import sys

    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="æ•°æ®æ¸…æ´—å·¥å…·")
    parser.add_argument("stock_code", nargs="?", default="000001", help="è‚¡ç¥¨ä»£ç ")
    parser.add_argument("--post-backtest", action="store_true",
                       help="å›æµ‹åæ¨¡å¼ï¼šå°†æ•°æ®æŒ‰æ—¥æœŸå€’åºæ’åˆ—")
    parser.add_argument("-p", "--post", action="store_true",
                       help="å›æµ‹åæ¨¡å¼çš„ç®€å†™ (ç­‰åŒäº --post-backtest)")

    # è§£æå‚æ•°
    if len(sys.argv) == 1:
        parser.print_help()
        sys.exit(0)

    args = parser.parse_args()
    stock_code = args.stock_code
    post_backtest_mode = args.post_backtest or args.post

    # åˆ›å»ºæ•°æ®æ¸…æ´—å™¨
    cleaner = EnhancedDataCleaner()

    # æ‰§è¡Œæ¸…æ´—
    result = cleaner.clean_stock_data(stock_code, post_backtest_mode=post_backtest_mode)

    mode_text = "å›æµ‹åæ•°æ®å€’åºå¤„ç†" if post_backtest_mode else "æ•°æ®æ¸…æ´—"
    if result:
        print(f"âœ… è‚¡ç¥¨ {stock_code} {mode_text}å®Œæˆ")
    else:
        print(f"âŒ è‚¡ç¥¨ {stock_code} {mode_text}å¤±è´¥")