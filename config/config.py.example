#!/usr/bin/env python3
"""
ç»Ÿä¸€é…ç½®æ–‡ä»¶ - æ‰€æœ‰é…ç½®é›†ä¸­ç®¡ç†
ä½¿ç”¨PythonåŸç”Ÿæ ¼å¼ï¼Œé¿å…å†—ä½™
"""

from dataclasses import dataclass
from typing import List, Optional
from pathlib import Path
from datetime import datetime
import asyncio
import threading


# Gemini API Keys - ç”¨äºè½®æ¢
GEMINI_API_KEYS = [
    "YOUR_API_KEY_HERE",
    "YOUR_API_KEY_HERE",
    "YOUR_API_KEY_HERE",
    "YOUR_API_KEY_HERE",
    "YOUR_API_KEY_HERE"
]

# API Key è½®æ¢ç®¡ç†å™¨
class APIKeyRotator:
    """API Key è½®æ¢ç®¡ç†å™¨ - ç¡®ä¿æ¯æ¬¡ä½¿ç”¨ä¸åŒçš„ key"""
    
    def __init__(self, api_keys: List[str]):
        self.api_keys = api_keys
        self.current_index = 0
        self.lock = threading.Lock()
    
    def get_next_key(self) -> str:
        """è·å–ä¸‹ä¸€ä¸ª API keyï¼ˆè½®æ¢ä½¿ç”¨ï¼‰"""
        with self.lock:
            key = self.api_keys[self.current_index]
            self.current_index = (self.current_index + 1) % len(self.api_keys)
            return key

# åˆ›å»ºå…¨å±€è½®æ¢å™¨
_api_key_rotator = APIKeyRotator(GEMINI_API_KEYS)


@dataclass
class Config:
    """ç»Ÿä¸€é…ç½®ç±»"""

    # Webé…ç½®
    web_host: str = "0.0.0.0"
    web_port: int = 8501
    web_debug: bool = False
    web_theme: str = "light"
    web_wide_mode: bool = True
    web_enable_caching: bool = True

    # ç³»ç»Ÿé…ç½®
    app_name: str = "Aè‚¡åˆ†æç³»ç»Ÿ"
    app_version: str = "2.0.0"
    target_stocks: List[str] = None
    request_delay: float = 2.0
    historical_start_date: str = "20210101"
    enable_parallel: bool = True
    max_workers: int = 3  # Macç³»ç»Ÿæ¨èå¹¶å‘æ•°ä¸º3ï¼Œé¿å…APIé™æµ

    # APIé…ç½®
    api_key: str = ""  # ä¸å†ä½¿ç”¨å•ä¸€ keyï¼Œé€šè¿‡è½®æ¢å™¨è·å–
    api_base_url: str = "https://123asxcvh-gemini-94.deno.dev/v1"
    api_timeout: int = 90
    api_max_retries: int = 5  # å¢åŠ é‡è¯•æ¬¡æ•°ä»¥æ”¯æŒ key è½®æ¢

    # AIæ¨¡å‹é…ç½®
    model_name: str = "gemini-2.5-flash"
    model_temperature: float = 0.2
    model_max_tokens: int = None

    # å›æµ‹é…ç½®
    initial_capital: float = 100000.0
    commission_rate: float = 0.0003
    slippage_rate: float = 0.001
    benchmark: str = "000300.SH"

    # æŠ€æœ¯æŒ‡æ ‡é…ç½®
    kdj_fastk_period: int = 9
    kdj_slowk_period: int = 3
    kdj_slowd_period: int = 3
    macd_fast_period: int = 12
    macd_slow_period: int = 26
    macd_signal_period: int = 9
    bbi_periods: List[int] = None
    boll_period: int = 20
    boll_nbdev: int = 2
    rsi_period: int = 14
    volume_ma_period: int = 20

    def __post_init__(self):
        """åˆå§‹åŒ–åå¤„ç†"""
        if self.target_stocks is None:
            self.target_stocks = ["000001", "000002", "000858", "002415", "600036", "600519"]
        if self.bbi_periods is None:
            self.bbi_periods = [3, 6, 12, 24]

    # è·¯å¾„ç®¡ç†
    @property
    def project_root(self) -> Path:
        """é¡¹ç›®æ ¹ç›®å½•"""
        return Path(__file__).parent.parent

    @property
    def data_dir(self) -> Path:
        """æ•°æ®ç›®å½•"""
        return self.project_root / "data"

    @property
    def stocks_dir(self) -> Path:
        """è‚¡ç¥¨æ•°æ®ç›®å½•"""
        return self.data_dir / "stocks"

    @property
    def cleaned_stocks_dir(self) -> Path:
        """æ¸…æ´—åè‚¡ç¥¨æ•°æ®ç›®å½•"""
        return self.data_dir / "cleaned_stocks"

    @property
    def ai_reports_dir(self) -> Path:
        """AIæŠ¥å‘Šç›®å½•"""
        return self.data_dir / "ai_reports"

    @property
    def cache_dir(self) -> Path:
        """ç¼“å­˜ç›®å½•"""
        return self.data_dir / "cache"

    @property
    def market_data_dir(self) -> Path:
        """å¸‚åœºæ•°æ®ç›®å½•"""
        return self.data_dir / "market_data"

    def get_stock_dir(self, stock_code: str, cleaned: bool = False) -> Path:
        """è·å–è‚¡ç¥¨ç›®å½•"""
        base_dir = self.cleaned_stocks_dir if cleaned else self.stocks_dir
        return base_dir / stock_code

    def get_stock_quotes_csv(self, stock_code: str, cleaned: bool = False) -> Path:
        """è·å–è‚¡ç¥¨è¡Œæƒ…CSVè·¯å¾„"""
        return self.get_stock_dir(stock_code, cleaned) / "historical_quotes.csv"

    def get_stock_file_path(self, stock_code: str, filename: str, cleaned: bool = False) -> Path:
        """è·å–è‚¡ç¥¨æ•°æ®æ–‡ä»¶è·¯å¾„"""
        return self.get_stock_dir(stock_code, cleaned) / filename

    def ensure_dir(self, path: Path) -> Path:
        """ç¡®ä¿ç›®å½•å­˜åœ¨"""
        path.mkdir(parents=True, exist_ok=True)
        return path

    def get_stocks_dir(self, cleaned: bool = False) -> Path:
        """è·å–è‚¡ç¥¨æ•°æ®ç›®å½•"""
        return self.cleaned_stocks_dir if cleaned else self.stocks_dir

    def get_ai_reports_dir(self) -> Path:
        """è·å–AIæŠ¥å‘Šç›®å½•"""
        return self.ai_reports_dir

    def get_market_data_dir(self, cleaned: bool = False) -> Path:
        """è·å–å¸‚åœºæ•°æ®ç›®å½•"""
        if cleaned:
            return self.data_dir / "cleaned_market_data"
        return self.market_data_dir

    def get_index_stocks_dir(self) -> Path:
        """è·å–æŒ‡æ•°è‚¡ç¥¨ç›®å½•"""
        return self.data_dir / "index_stocks"

    def get_concept_stocks_dir(self) -> Path:
        """è·å–æ¦‚å¿µè‚¡ç¥¨ç›®å½•"""
        return self.data_dir / "concept_stocks"

    def get_industry_stocks_dir(self) -> Path:
        """è·å–è¡Œä¸šè‚¡ç¥¨ç›®å½•"""
        return self.data_dir / "industry_stocks"

    def get_config_file_path(self, config_name: str = None) -> Path:
        """è·å–é…ç½®æ–‡ä»¶è·¯å¾„ï¼ˆå‘åå…¼å®¹æ–¹æ³•ï¼‰"""
        return self.project_root / "config" / f"{config_name or 'config'}.py"

    # å›æµ‹ç»“æœè·¯å¾„ç®¡ç†
    def get_strategy_results_dir(self, stock_code: str, strategy_type: str, strategy_name: str) -> Path:
        """è·å–ç­–ç•¥å›æµ‹ç»“æœç›®å½•
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            strategy_type: ç­–ç•¥ç±»å‹ (Single/Double/Triple)
            strategy_name: ç­–ç•¥åç§°
            
        Returns:
            ç­–ç•¥ç»“æœç›®å½•è·¯å¾„
        """
        base_dir = self.get_stock_dir(stock_code, cleaned=True) / "results" / strategy_type / strategy_name
        return self.ensure_dir(base_dir)

    def get_strategy_type_dir(self, stock_code: str, strategy_type: str) -> Path:
        """è·å–ç­–ç•¥ç±»å‹ç›®å½•
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            strategy_type: ç­–ç•¥ç±»å‹ (Single/Double/Triple)
            
        Returns:
            ç­–ç•¥ç±»å‹ç›®å½•è·¯å¾„
        """
        base_dir = self.get_stock_dir(stock_code, cleaned=True) / "results" / strategy_type
        return self.ensure_dir(base_dir)

    def get_stock_strategy_summary_path(self, stock_code: str) -> Path:
        """è·å–è‚¡ç¥¨ç­–ç•¥æ±‡æ€»æ–‡ä»¶è·¯å¾„
        
        Args:
            stock_code: è‚¡ç¥¨ä»£ç 
            
        Returns:
            æ±‡æ€»æ–‡ä»¶è·¯å¾„
        """
        results_dir = self.get_stock_dir(stock_code, cleaned=True) / "results"
        self.ensure_dir(results_dir)
        return results_dir / "strategy_summary.csv"

    # AIåˆ†æé…ç½®
    @property
    def supported_stock_analysis_types(self) -> List[str]:
        """æ”¯æŒçš„ä¸ªäººè‚¡ç¥¨åˆ†æç±»å‹"""
        return [
            "company_profile",
            "balance_sheet_analysis",      # èµ„äº§è´Ÿå€ºè¡¨åˆ†æ
            "income_statement_analysis",    # åˆ©æ¶¦è¡¨åˆ†æ
            "cash_flow_analysis",          # ç°é‡‘æµé‡è¡¨åˆ†æ
            "financial_indicators_analysis", # è´¢åŠ¡æŒ‡æ ‡åˆ†æ
            "technical_analysis",
            "intraday_trading"
        ]

    @property
    def supported_market_analysis_types(self) -> List[str]:
        """æ”¯æŒçš„å¸‚åœºåˆ†æç±»å‹"""
        return [
            "fund_flow_concept",
            "fund_flow_industry",
            "fund_flow_individual",
            "zt_pool",
            "news_main_cx",
            "market_activity_legu"
        ]

    @property
    def analysis_file_mapping(self) -> dict:
        """åˆ†æç±»å‹ä¸æ•°æ®æ–‡ä»¶çš„æ˜ å°„"""
        return {
            "company_profile": ["company_profile.csv", "main_business_composition.csv"],
            "balance_sheet_analysis": ["balance_sheet.csv"],      # èµ„äº§è´Ÿå€ºè¡¨åˆ†æ
            "income_statement_analysis": ["income_statement.csv"],    # åˆ©æ¶¦è¡¨åˆ†æ
            "cash_flow_analysis": ["cash_flow_statement.csv"],          # ç°é‡‘æµé‡è¡¨åˆ†æ
            "financial_indicators_analysis": ["financial_indicators.csv"], # è´¢åŠ¡æŒ‡æ ‡åˆ†æ
            "technical_analysis": ["historical_quotes.csv"],
            "intraday_trading": ["intraday_data.csv"]
        }

    @property
    def market_analysis_file_mapping(self) -> dict:
        """å¸‚åœºåˆ†æç±»å‹ä¸æ•°æ®æ–‡ä»¶çš„æ˜ å°„"""
        return {
            "fund_flow_concept": ["fund_flow_concept.csv"],
            "fund_flow_industry": ["fund_flow_industry.csv"],
            "fund_flow_individual": ["fund_flow_individual.csv"],
            "zt_pool": ["zt_pool.csv"],
            "news_main_cx": ["news_main_cx.csv"],
            "market_activity_legu": ["market_activity_legu.csv"]
        }


# å…¨å±€é…ç½®å®ä¾‹
config = Config()


# ä¾¿æ·è®¿é—®å‡½æ•°
def get_web_port() -> int:
    return config.web_port

def get_api_key() -> str:
    """è·å–ä¸‹ä¸€ä¸ªè½®æ¢çš„ API key"""
    return _api_key_rotator.get_next_key()

def get_target_stocks() -> List[str]:
    return config.target_stocks

def get_project_root() -> Path:
    return config.project_root

def get_data_dir() -> Path:
    return config.data_dir


# ç­–ç•¥é…ç½®ï¼ˆç®€åŒ–ç‰ˆï¼‰
class StrategyConfigs:
    """ç­–ç•¥é…ç½®"""

    # KDJç­–ç•¥
    kdj_j_buy_threshold = 0
    kdj_j_sell_threshold = 100
    kdj_signal_cooldown_days = 3

    # å‘¨çº¿KDJç­–ç•¥
    weekly_kdj_j_buy_threshold = 10
    weekly_kdj_signal_cooldown_days = 5

    # RSIç­–ç•¥
    rsi_oversold = 30
    rsi_overbought = 70
    rsi_signal_cooldown_days = 3

    # MACDç­–ç•¥
    macd_signal_cooldown_days = 3

    # BBIç­–ç•¥
    bbi_signal_cooldown_days = 3

    # BOLLç­–ç•¥
    boll_signal_cooldown_days = 3

    # æˆäº¤é‡çªç ´ç­–ç•¥
    volume_multiplier = 2.0
    volume_signal_cooldown_days = 3

    # æ··åˆç­–ç•¥
    hybrid_observation_timeout_days = 5
    hybrid_signal_cooldown_days = 5

    # ä¸‰æŒ‡æ ‡ç­–ç•¥
    triple_observation_timeout_days = 5
    triple_validation_timeout_days = 3
    triple_signal_cooldown_days = 5

    @classmethod
    def get_strategy_config(cls, strategy_name: str) -> dict:
        """è·å–ç­–ç•¥é…ç½®"""
        # åŸºç¡€é…ç½®æ¨¡æ¿
        base_daily_kdj = {
            "j_buy_threshold": cls.kdj_j_buy_threshold,
            "j_sell_threshold": cls.kdj_j_sell_threshold,
            "signal_cooldown_days": cls.kdj_signal_cooldown_days
        }
        base_weekly_kdj = {
            "j_buy_threshold": cls.weekly_kdj_j_buy_threshold,
            "j_sell_threshold": cls.kdj_j_sell_threshold,
            "signal_cooldown_days": cls.weekly_kdj_signal_cooldown_days
        }
        
        configs = {
            "Base_DailyKDJ": base_daily_kdj,
            "Base_WeeklyKDJ": base_weekly_kdj,
            "Base_RSI": {
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "signal_cooldown_days": cls.rsi_signal_cooldown_days
            },
            "Base_MACD": {
                "signal_cooldown_days": cls.macd_signal_cooldown_days
            },
            "Base_BBI": {
                "signal_cooldown_days": cls.bbi_signal_cooldown_days
            },
            "Base_BOLL": {
                "signal_cooldown_days": cls.boll_signal_cooldown_days
            },
            "Base_VolumeBreakout": {
                "volume_multiplier": cls.volume_multiplier,
                "signal_cooldown_days": cls.volume_signal_cooldown_days
            },
            # Hybrid ç­–ç•¥é…ç½® (DoubleæŒ‡æ ‡ç­–ç•¥)
            "Hybrid_DailyKDJ_BBI": {
                **base_daily_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_DailyKDJ_BOLL": {
                **base_daily_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_DailyKDJ_MACD": {
                **base_daily_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_DailyKDJ_RSI": {
                **base_daily_kdj,
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_DailyKDJ_Volume": {
                **base_daily_kdj,
                "volume_multiplier": cls.volume_multiplier,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_WeeklyKDJ_BBI": {
                **base_weekly_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_WeeklyKDJ_BOLL": {
                **base_weekly_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_WeeklyKDJ_MACD": {
                **base_weekly_kdj,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_WeeklyKDJ_RSI": {
                **base_weekly_kdj,
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            "Hybrid_WeeklyKDJ_Volume": {
                **base_weekly_kdj,
                "volume_multiplier": cls.volume_multiplier,
                "observation_timeout_days": cls.hybrid_observation_timeout_days,
                "signal_cooldown_days": cls.hybrid_signal_cooldown_days
            },
            # Triple ç­–ç•¥é…ç½® (ä¸‰æŒ‡æ ‡ç­–ç•¥)
            "Triple_DailyKDJ_BOLL_RSI": {
                **base_daily_kdj,
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            },
            "Triple_DailyKDJ_BOLL_MACD": {
                **base_daily_kdj,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            },
            "Triple_DailyKDJ_BBI_RSI": {
                **base_daily_kdj,
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            },
            "Triple_DailyKDJ_BBI_MACD": {
                **base_daily_kdj,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            },
            "Triple_WeeklyKDJ_BOLL_RSI": {
                **base_weekly_kdj,
                "rsi_oversold": cls.rsi_oversold,
                "rsi_overbought": cls.rsi_overbought,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            },
            "Triple_WeeklyKDJ_BOLL_MACD": {
                **base_weekly_kdj,
                "observation_timeout_days": cls.triple_observation_timeout_days,
                "validation_timeout_days": cls.triple_validation_timeout_days,
                "signal_cooldown_days": cls.triple_signal_cooldown_days
            }
        }
        return configs.get(strategy_name, {})


# ç­–ç•¥é…ç½®å®ä¾‹
strategy_configs = StrategyConfigs()




# ç¼“å­˜ç®¡ç†å™¨
class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨ï¼Œç”¨äºç®¡ç†è‚¡ç¥¨åˆ†æçš„ç¼“å­˜"""

    def __init__(self, cache_dir: Path = None, cache_days: int = 7):
        """åˆå§‹åŒ–ç¼“å­˜ç®¡ç†å™¨"""
        if cache_dir is None:
            cache_dir = config.cache_dir
        self.cache_dir = cache_dir
        self.cache_days = cache_days
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._cleanup_expired_cache()

    def _generate_cache_key(self, symbol: str, filename: str, system_role: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        import hashlib
        content = f"{symbol}_{filename}_{system_role}"
        return hashlib.md5(content.encode()).hexdigest()

    def _get_cache_file_path(self, symbol: str, cache_key: str) -> Path:
        """è·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„ï¼ŒæŒ‰è‚¡ç¥¨ä»£ç åˆ†ç›®å½•"""
        symbol_cache_dir = self.cache_dir / symbol
        symbol_cache_dir.mkdir(exist_ok=True)
        return symbol_cache_dir / f"{cache_key}.json"

    def _is_cache_valid(self, cache_file_path: Path) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
        if not cache_file_path.exists():
            return False

        file_mtime = datetime.fromtimestamp(cache_file_path.stat().st_mtime)
        cache_age = datetime.now() - file_mtime
        return cache_age.days < self.cache_days

    def _cleanup_expired_cache(self):
        """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜æ–‡ä»¶"""
        current_time = datetime.now()

        for symbol_dir in self.cache_dir.iterdir():
            if not symbol_dir.is_dir():
                continue

            for cache_file in symbol_dir.glob("*.json"):
                file_mtime = datetime.fromtimestamp(cache_file.stat().st_mtime)
                if (current_time - file_mtime).days >= self.cache_days:
                    cache_file.unlink()

    async def load_from_cache(self, symbol: str, filename: str, system_role: str):
        """ä»ç¼“å­˜åŠ è½½æ•°æ®ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        import json
        import aiofiles
        
        cache_key = self._generate_cache_key(symbol, filename, system_role)
        cache_file_path = self._get_cache_file_path(symbol, cache_key)

        if not self._is_cache_valid(cache_file_path):
            return None

        try:
            async with aiofiles.open(cache_file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                cached_data = json.loads(content)
                return cached_data.get("data", cached_data)
        except Exception as e:
            print(f"âš ï¸ è¯»å–ç¼“å­˜å¤±è´¥ {cache_file_path}: {e}")
            return None

    async def save_to_cache(self, symbol: str, filename: str, system_role: str, data) -> bool:
        """ä¿å­˜æ•°æ®åˆ°ç¼“å­˜ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        import json
        import aiofiles
        
        cache_key = self._generate_cache_key(symbol, filename, system_role)
        cache_file_path = self._get_cache_file_path(symbol, cache_key)

        cache_data = {
            "symbol": symbol,
            "filename": filename,
            "system_role": system_role,
            "cached_at": datetime.now().isoformat(),
            "data": data
        }

        try:
            async with aiofiles.open(cache_file_path, 'w', encoding='utf-8') as f:
                await f.write(json.dumps(cache_data, ensure_ascii=False, indent=2))
            return True
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ç¼“å­˜å¤±è´¥ {cache_file_path}: {e}")
            return False

    def clear_cache(self, symbol: str = None):
        """æ¸…é™¤ç¼“å­˜"""
        import shutil

        if symbol:
            symbol_cache_dir = self.cache_dir / symbol
            if symbol_cache_dir.exists():
                shutil.rmtree(symbol_cache_dir)
        else:
            shutil.rmtree(self.cache_dir)
            self.cache_dir.mkdir(parents=True, exist_ok=True)

    def get_cache_stats(self):
        """è·å–ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_files = 0
        total_size = 0
        symbol_stats = {}

        for symbol_dir in self.cache_dir.iterdir():
            if not symbol_dir.is_dir():
                continue

            symbol_files = list(symbol_dir.glob("*.json"))
            symbol_size = sum(f.stat().st_size for f in symbol_files)

            symbol_stats[symbol_dir.name] = {
                "files_count": len(symbol_files),
                "size_bytes": symbol_size,
                "size_mb": round(symbol_size / (1024 * 1024), 2)
            }

            total_files += len(symbol_files)
            total_size += symbol_size

        return {
            "total_files": total_files,
            "total_size_bytes": total_size,
            "total_size_mb": round(total_size / (1024 * 1024), 2),
            "symbols_count": len(symbol_stats),
            "symbol_stats": symbol_stats
        }


# AIåˆ†æå™¨åŸºç±»
class AsyncAIAnalyzerBase:
    """å¼‚æ­¥AIåˆ†æå™¨åŸºç±»"""

    def __init__(self, model_name: str = None):
        self.model_name = model_name or config.model_name
        # ä¸å†ä½¿ç”¨å•ä¸€ api_keyï¼Œé€šè¿‡è½®æ¢å™¨åœ¨æ¯æ¬¡è°ƒç”¨æ—¶è·å–
        self.api_key = None  # ä¿ç•™æ­¤å­—æ®µç”¨äºå…¼å®¹æ€§ï¼Œä½†å®é™…ä½¿ç”¨è½®æ¢å™¨
        self.base_url = config.api_base_url
        self.timeout = config.api_timeout
        self.prompt_manager = None

    async def __aenter__(self):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨å…¥å£"""
        from src.ai_analysis.prompts.prompt_manager import PromptManager
        self.prompt_manager = PromptManager()
        return self

    async def __aexit__(self, exc_type, exc_val, exc_tb):
        """å¼‚æ­¥ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡º"""
        # å¦‚æœéœ€è¦æ¸…ç†èµ„æºï¼Œåœ¨è¿™é‡Œæ·»åŠ 
        return False

    async def _call_ai_api_with_retry(self, messages):
        """è°ƒç”¨AI APIï¼ˆå¸¦ API key è½®æ¢çš„é‡è¯•æœºåˆ¶ï¼‰"""
        import httpx
        import json

        request_body = {
            "model": self.model_name,
            "messages": messages,
            "temperature": config.model_temperature
        }

        max_retries = config.api_max_retries
        
        for attempt in range(max_retries):
            # è·å–ä¸‹ä¸€ä¸ª API keyï¼ˆè½®æ¢ä½¿ç”¨ï¼‰
            current_api_key = _api_key_rotator.get_next_key()
            
            headers = {
                "Authorization": f"Bearer {current_api_key}",
                "Content-Type": "application/json"
            }

            try:
                async with httpx.AsyncClient(timeout=self.timeout) as client:
                    response = await client.post(
                        f"{self.base_url}/chat/completions",
                        json=request_body,
                        headers=headers
                    )

                    if response.status_code == 200:
                        result = response.json()
                        if "choices" in result and len(result["choices"]) > 0:
                            return result["choices"][0]["message"]["content"]
                        else:
                            # å“åº”æ ¼å¼å¼‚å¸¸ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                            if attempt < max_retries - 1:
                                await asyncio.sleep(1)
                                continue
                            return None
                    elif response.status_code == 401:
                        # è®¤è¯å¤±è´¥ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                        if attempt < max_retries - 1:
                            await asyncio.sleep(1)
                            continue
                        return None
                    elif response.status_code == 429:
                        # é¢‘ç‡é™åˆ¶ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                        if attempt < max_retries - 1:
                            await asyncio.sleep(3)
                            continue
                        return None
                    elif response.status_code >= 500:
                        # æœåŠ¡å™¨é”™è¯¯ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                        if attempt < max_retries - 1:
                            await asyncio.sleep(2)
                            continue
                        return None
                    else:
                        # å…¶ä»–é”™è¯¯ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                        if attempt < max_retries - 1:
                            await asyncio.sleep(1)
                            continue
                        return None

            except (httpx.TimeoutException, httpx.ConnectError) as e:
                # ç½‘ç»œé”™è¯¯ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                if attempt < max_retries - 1:
                    await asyncio.sleep(2)
                    continue
            except Exception as e:
                # å…¶ä»–é”™è¯¯ï¼Œæ¢ä¸‹ä¸€ä¸ª key é‡è¯•
                if attempt < max_retries - 1:
                    await asyncio.sleep(1)
                    continue

        return None

    async def test_api_connection(self):
        """æµ‹è¯•APIè¿æ¥æ˜¯å¦æ­£å¸¸"""
        test_messages = [
            {"role": "user", "content": "æµ‹è¯•è¿æ¥ï¼Œè¯·å›å¤'è¿æ¥æˆåŠŸ'"}
        ]

        print("ğŸ” æµ‹è¯•APIè¿æ¥...")
        result = await self._call_ai_api_with_retry(test_messages)

        if result:
            print("âœ… APIè¿æ¥æµ‹è¯•æˆåŠŸ")
            return True
        else:
            print("âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥")
            return False


# æ•°æ®å¤„ç†å™¨
class DataProcessor:
    """æ•°æ®å¤„ç†å™¨ï¼ˆé€šç”¨ç‰ˆï¼‰"""

    @staticmethod
    async def read_csv_file_async(file_path: str):
        """å¼‚æ­¥è¯»å–CSVæ–‡ä»¶"""
        import aiofiles
        from io import StringIO
        import pandas as pd

        try:
            async with aiofiles.open(file_path, 'r', encoding='utf-8') as f:
                content = await f.read()
                return pd.read_csv(StringIO(content))
        except Exception as e:
            print(f"è¯»å–æ–‡ä»¶å¤±è´¥ {file_path}: {e}")
            return None

    @staticmethod
    async def filter_data_by_time(df, filename: str):
        """æŒ‰æ—¶é—´è¿‡æ»¤æ•°æ®ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return df

    @staticmethod
    def filter_industry_comparison_columns(df, filename: str):
        """è¿‡æ»¤è¡Œä¸šå¯¹æ¯”æ•°æ®åˆ—ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return df

    @staticmethod
    def filter_market_data_columns(df, filename: str):
        """è¿‡æ»¤å¸‚åœºæ•°æ®åˆ—ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return df

    @staticmethod
    async def load_company_context(stock_code: str, data_dir: str, cache: dict):
        """åŠ è½½å…¬å¸ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        try:
            # æ£€æŸ¥ç¼“å­˜
            cache_key = f"company_context_{stock_code}"
            if cache_key in cache:
                return cache[cache_key]

            # å°è¯•ä»å…¬å¸æ¦‚å†µæ–‡ä»¶åŠ è½½
            import aiofiles
            import pandas as pd
            from pathlib import Path
            from io import StringIO

            company_info_file = Path(data_dir) / stock_code / "company_profile.csv"
            if company_info_file.exists():
                async with aiofiles.open(company_info_file, 'r', encoding='utf-8') as f:
                    content = await f.read()
                    # è¯»å–CSVæ•°æ®
                    df = pd.read_csv(StringIO(content))

                    # æå–å…³é”®å…¬å¸ä¿¡æ¯
                    company_info = {}
                    if not df.empty and 'å­—æ®µå' in df.columns and 'å­—æ®µå€¼' in df.columns:
                        # å¤„ç†é”®å€¼å¯¹æ ¼å¼
                        company_info = dict(zip(df['å­—æ®µå'], df['å­—æ®µå€¼']))
      
                    company_context = f"""
å…¬å¸åç§°ï¼š{company_info.get('å…¬å¸åç§°', stock_code)}
è‚¡ç¥¨ä»£ç ï¼š{stock_code}
æ‰€å±è¡Œä¸šï¼š{company_info.get('æ‰€å±è¡Œä¸š', 'æœªçŸ¥')}
ä¸»è¥ä¸šåŠ¡ï¼š{company_info.get('ä¸»è¥ä¸šåŠ¡', 'æœªçŸ¥')}
å…¬å¸ç®€ä»‹ï¼š{company_info.get('æœºæ„ç®€ä»‹', 'æš‚æ— å…¬å¸è¯¦ç»†ä¿¡æ¯')}

é‡è¦æé†’ï¼šä¸Šè¿°å…¬å¸ä¿¡æ¯æ¥æºäºæä¾›çš„company_profile.csvæ–‡ä»¶ï¼Œè¯·åŠ¡å¿…åœ¨åˆ†ææŠ¥å‘Šä¸­ä½¿ç”¨è¿™äº›ä¿¡æ¯ï¼Œä¸è¦æ˜¾ç¤ºä¸º"æœªçŸ¥"ã€‚
"""
                    cache[cache_key] = company_context
                    return company_context

            # å¦‚æœæ²¡æœ‰æ–‡ä»¶ï¼Œè¿”å›åŸºæœ¬ä¿¡æ¯
            fallback_context = f"""
å…¬å¸åç§°ï¼š{stock_code}
è‚¡ç¥¨ä»£ç ï¼š{stock_code}
æ‰€å±è¡Œä¸šï¼šæœªçŸ¥
ä¸»è¥ä¸šåŠ¡ï¼šæœªçŸ¥
å…¬å¸ç®€ä»‹ï¼šæš‚æ— å…¬å¸è¯¦ç»†ä¿¡æ¯
"""
            cache[cache_key] = fallback_context
            return fallback_context

        except Exception as e:
            print(f"âš ï¸ åŠ è½½å…¬å¸ä¸Šä¸‹æ–‡å¤±è´¥ {stock_code}: {e}")
            return f"å…¬å¸ {stock_code} çš„åŸºæœ¬ä¿¡æ¯ï¼ˆåŠ è½½å¤±è´¥ï¼‰"

    @staticmethod
    async def load_market_context(market_type: str, data_dir: str, cache: dict):
        """åŠ è½½å¸‚åœºä¸Šä¸‹æ–‡ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        return f"å¸‚åœº {market_type} çš„åŸºæœ¬ä¿¡æ¯"


# åˆ›å»ºç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()

# å¸¸é‡å¯¼å‡ºï¼ˆå‘åå…¼å®¹ï¼‰
MODEL_NAME = config.model_name
MAX_CONCURRENCY = config.max_workers


# å¯¼å‡º
__all__ = [
    'config', 'Config',
    'get_web_port', 'get_api_key', 'get_target_stocks',
    'get_project_root', 'get_data_dir',
    'strategy_configs', 'StrategyConfigs',
    'CacheManager', 'cache_manager',
    'AsyncAIAnalyzerBase', 'DataProcessor',
    'MODEL_NAME', 'MAX_CONCURRENCY'
]