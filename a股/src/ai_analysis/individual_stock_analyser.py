#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
ç®€åŒ–AIåˆ†æå™¨
åªä¿ç•™æ ¸å¿ƒåŠŸèƒ½
"""

import sys
from pathlib import Path
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

import pandas as pd
import asyncio
import aiofiles
import time
from typing import Dict, Any, List
from datetime import datetime

from config import (
    config, cache_manager, MODEL_NAME, MAX_CONCURRENCY,
    AsyncAIAnalyzerBase, DataProcessor
)
from src.ai_analysis.prompts.prompt_manager import PromptManager

def get_config():
    # ä»é…ç½®ç³»ç»Ÿè·å–åˆ†æç±»å‹å’Œæ˜ å°„
    supported_analysis_types = config.supported_stock_analysis_types
    return config.cleaned_stocks_dir, config.ai_reports_dir, supported_analysis_types, MODEL_NAME

def run_main(main_func):
    if sys.platform == "win32":
        asyncio.set_event_loop_policy(asyncio.WindowsSelectorEventLoopPolicy())
    try:
        asyncio.run(main_func())
    except KeyboardInterrupt:
        print("ç”¨æˆ·ä¸­æ–­æ“ä½œ")

class AsyncStockAIAnalyzer(AsyncAIAnalyzerBase):
    """ç®€åŒ–å¼‚æ­¥è‚¡ç¥¨AIåˆ†æå™¨"""

    # ç¼“å­˜åˆ†æç±»å‹é…ç½®
    CACHEABLE_ANALYSIS_TYPES = [
        "company_profile",
        "balance_sheet_analysis",
        "income_statement_analysis",
        "cash_flow_analysis",
        "financial_indicators_analysis"
    ]

    # æ•°æ®é‡‡æ ·é™åˆ¶é…ç½®
    DATA_LIMITS = {
        "technical_analysis": 100,
        "intraday_trading": 100
    }

    def __init__(self, model_name: str = None):
        super().__init__(model_name)
        # ç«‹å³åˆå§‹åŒ–prompt_managerï¼Œä¸ä¾èµ–å¼‚æ­¥ä¸Šä¸‹æ–‡
        from src.ai_analysis.prompts.prompt_manager import PromptManager
        self.prompt_manager = PromptManager()

    async def _process_single_analysis_async(self, stock_code: str, analysis_type: str,
                                            data_dir: str, output_path: str) -> bool:
        """å¼‚æ­¥å¤„ç†å•ä¸ªåˆ†æä»»åŠ¡"""

        # æ£€æŸ¥ç¼“å­˜
        if analysis_type in self.CACHEABLE_ANALYSIS_TYPES:
            analysis_result = await cache_manager.load_from_cache(stock_code, "multi_file_data", analysis_type)
            if analysis_result:
                await self._write_analysis_result(output_path, analysis_result)
                return True

        # ä»AIé…ç½®ç®¡ç†å™¨è·å–åˆ†æç±»å‹ä¸æ–‡ä»¶çš„æ˜ å°„
        analysis_config = config
        analysis_file_mapping = analysis_config.analysis_file_mapping

        # AIåˆ†æéœ€è¦çš„historical_quotesåˆ—
        HISTORICAL_QUOTES_AI_COLUMNS = [
            'æ—¥æœŸ', 'å¼€ç›˜', 'æ”¶ç›˜', 'æœ€é«˜', 'æœ€ä½', 'æˆäº¤é‡',
            'MACD_HIST', 'DAILY_KDJ_J', 'RSI', 'BBI', 'MA5', 'MA10'
        ]

        # è¯»å–æ–‡ä»¶æ•°æ®
        file_data = {}
        data_dir_path = Path(data_dir)  # ç¡®ä¿ä½¿ç”¨ Path å¯¹è±¡å¤„ç†è·¯å¾„
        for filename in analysis_file_mapping[analysis_type]:
            file_path = str(data_dir_path / filename)  # ä½¿ç”¨ Path æ‹¼æ¥ï¼Œç„¶åè½¬ä¸ºå­—ç¬¦ä¸²
            df = await DataProcessor.read_csv_file_async(file_path)
            if df is not None:
                # å¯¹äºhistorical_quotes.csvï¼Œåªè¯»å–AIåˆ†æéœ€è¦çš„åˆ—
                if filename == 'historical_quotes.csv' and len(df.columns) > 12:
                    # æ£€æŸ¥æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„åˆ—
                    available_columns = [col for col in HISTORICAL_QUOTES_AI_COLUMNS if col in df.columns]
                    if available_columns:
                        df = df[available_columns]
                        print(f"ğŸ“Š {stock_code} historical_quotes.csv å·²ç­›é€‰ä¸º {len(available_columns)} ä¸ªAIåˆ†æåˆ—")

                df_filtered = await DataProcessor.filter_data_by_time(df, filename)
                file_data[filename] = df_filtered

        # æ„å»ºæ•°æ®æ‘˜è¦
        summary_parts = [f"=== {stock_code} {analysis_type.upper()} æ•°æ®åˆ†æ ==="]

        # ä¸ºæ‰€æœ‰åˆ†æç±»å‹æ·»åŠ å…¬å¸ä¸Šä¸‹æ–‡ä¿¡æ¯
        # ç›´æ¥ä»CSVæå–å…¬å¸ä¿¡æ¯å¹¶æ‹¼æ¥åˆ°promptä¸­
        company_info = await self._extract_company_info_csv(stock_code, data_dir)

        if company_info:
            # ç›´æ¥å°†å…¬å¸ä¿¡æ¯æ‹¼æ¥åˆ°æ•°æ®æ‘˜è¦ä¸­
            summary_parts.append("### å…¬å¸èƒŒæ™¯ä¿¡æ¯")
            summary_parts.append(f"å…¬å¸åç§°ï¼š{company_info.get('å…¬å¸åç§°', stock_code)}")
            summary_parts.append(f"è‚¡ç¥¨ä»£ç ï¼š{stock_code}")
            summary_parts.append(f"æ‰€å±è¡Œä¸šï¼š{company_info.get('æ‰€å±è¡Œä¸š', 'æœªçŸ¥')}")
            summary_parts.append(f"ä¸»è¥ä¸šåŠ¡ï¼š{company_info.get('ä¸»è¥ä¸šåŠ¡', 'æœªçŸ¥')}")
            summary_parts.append(f"å…¬å¸ç®€ä»‹ï¼š{company_info.get('æœºæ„ç®€ä»‹', 'æš‚æ— å…¬å¸è¯¦ç»†ä¿¡æ¯')}")
            summary_parts.append("")
            summary_parts.append("é‡è¦è¯´æ˜ï¼šè¯·åœ¨åˆ†ææŠ¥å‘Šä¸­ä½¿ç”¨ä¸Šè¿°å…¬å¸ä¿¡æ¯ï¼Œä¸è¦æ˜¾ç¤ºä¸º'æœªçŸ¥'ã€‚")
            summary_parts.append("")

        # ä½¿ç”¨ç±»é…ç½®å¤„ç†æ•°æ®é‡‡æ ·é™åˆ¶
        limit = None if analysis_type == "fundamental_analysis" else self.DATA_LIMITS.get(analysis_type)

        for filename, df in file_data.items():
            summary_parts.append(f"=== {filename} ===")
            
            if limit and len(df) > limit:
                df_sample = df.head(limit)
                summary_parts.append(f"æ•°æ®æ€»è¡Œæ•°: {len(df)}, æ˜¾ç¤ºæœ€è¿‘{limit}è¡Œ:")
            else:
                df_sample = df
            
            # ä¼˜åŒ–ï¼šä½¿ç”¨ to_dict('records') ä»£æ›¿ iterrows()ï¼Œæ€§èƒ½æ›´å¥½
            rows_data = df_sample.to_dict('records')
            for row in rows_data:
                row_info = " | ".join([f"{col}: {val}" for col, val in row.items() if pd.notna(val)])
                summary_parts.append(row_info)
            summary_parts.append("")

        combined_summary = "\n".join(summary_parts)

        # è·å–æç¤ºè¯å¹¶è°ƒç”¨AI API
        system_prompt, user_prompt = self.prompt_manager.get_stock_prompt(analysis_type, stock_code)
        messages = [
            {"role": "system", "content": self.prompt_manager.get_system_prompt(system_prompt)},
            {"role": "user", "content": f"{user_prompt}\n\n{combined_summary}"}
        ]
        
        print(f"ğŸ”„ {stock_code} {analysis_type} æ­£åœ¨è°ƒç”¨AI API...")
        analysis_result = await self._call_ai_api_with_retry(messages)

        # ç»Ÿä¸€å¤„ç†APIè°ƒç”¨ç»“æœ
        if not analysis_result:
            analysis_result = f"âŒ {analysis_type} AIåˆ†æå¤±è´¥ - APIè¿”å›ç©ºç»“æœ"
            self._log_api_result(stock_code, analysis_type, False)
        else:
            self._log_api_result(stock_code, analysis_type, True)

        # ä¿å­˜ç¼“å­˜å’Œå†™å…¥æ–‡ä»¶
        if analysis_result:
            if analysis_type in self.CACHEABLE_ANALYSIS_TYPES:
                await cache_manager.save_to_cache(stock_code, "multi_file_data", analysis_type, analysis_result)
            await self._write_analysis_result(output_path, analysis_result)

        return bool(analysis_result)

    async def _write_analysis_result(self, output_path: str, analysis_result: str):
        """å¼‚æ­¥å†™å…¥åˆ†æç»“æœåˆ°æ–‡ä»¶"""
        async with aiofiles.open(output_path, 'w', encoding='utf-8') as f:
            await f.write(analysis_result)

    def _log_api_result(self, stock_code: str, analysis_type: str, success: bool):
        """è®°å½•APIè°ƒç”¨ç»“æœ"""
        if success:
            print(f"âœ… {stock_code} {analysis_type} APIè°ƒç”¨æˆåŠŸ")
        else:
            print(f"âŒ {stock_code} {analysis_type} APIè°ƒç”¨è¿”å›ç©ºç»“æœ")

    async def _extract_company_info_csv(self, stock_code: str, data_dir: str) -> dict:
        """ç›´æ¥ä»CSVæå–å…¬å¸ä¿¡æ¯"""
        try:
            import pandas as pd
            from io import StringIO
            import aiofiles

            company_info_file = Path(data_dir) / "company_profile.csv"
            if not company_info_file.exists():
                return {}

            async with aiofiles.open(company_info_file, 'r', encoding='utf-8') as f:
                content = await f.read()
                df = pd.read_csv(StringIO(content))

                if df.empty or 'å­—æ®µå' not in df.columns or 'å­—æ®µå€¼' not in df.columns:
                    return {}

                # æå–å…³é”®å­—æ®µ
                company_info = dict(zip(df['å­—æ®µå'], df['å­—æ®µå€¼']))

                # è¿”å›å…³é”®å­—æ®µ
                return {
                    'å…¬å¸åç§°': company_info.get('å…¬å¸åç§°', stock_code),
                    'æ‰€å±è¡Œä¸š': company_info.get('æ‰€å±è¡Œä¸š', 'æœªçŸ¥'),
                    'ä¸»è¥ä¸šåŠ¡': company_info.get('ä¸»è¥ä¸šåŠ¡', 'æœªçŸ¥'),
                    'æœºæ„ç®€ä»‹': company_info.get('æœºæ„ç®€ä»‹', 'æš‚æ— å…¬å¸è¯¦ç»†ä¿¡æ¯')
                }

        except Exception as e:
            print(f"âš ï¸ æå–å…¬å¸ä¿¡æ¯å¤±è´¥ {stock_code}: {e}")
            return {}

    async def process_stock_analysis(self, stock_code: str, analysis_types: List[str],
                                   data_dir: str, output_dir: str) -> Dict[str, Any]:
        """å¤„ç†å•ä¸ªè‚¡ç¥¨çš„æ‰€æœ‰åˆ†æç±»å‹ - ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘"""
        stock_reports_dir = Path(output_dir) / stock_code
        stock_reports_dir.mkdir(parents=True, exist_ok=True)

        # åˆ›å»ºç»“æœè·Ÿè¸ª
        completed_tasks = {"successful": 0, "failed": 0}
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°ï¼ˆä»é…ç½®è·å–æœ€å¤§å¹¶å‘æ•°ï¼‰
        semaphore = asyncio.Semaphore(MAX_CONCURRENCY)

        async def process_type_with_semaphore(analysis_type: str):
            """å¼‚æ­¥å¤„ç†å•ä¸ªåˆ†æç±»å‹ï¼Œä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘"""
            async with semaphore:
                result = await self._process_single_analysis_async(
                    stock_code, analysis_type, data_dir, 
                    str(stock_reports_dir / f"{analysis_type}.md")
                )
                status = "æˆåŠŸ" if result else "å¤±è´¥"
                completed_tasks["successful" if result else "failed"] += 1
                print(f"{'âœ…' if result else 'âŒ'} {stock_code} {analysis_type} åˆ†æ{status}")
                return result

        # åˆ›å»ºå¹¶å‘ä»»åŠ¡ï¼ˆä½¿ç”¨ä¿¡å·é‡æ§åˆ¶ï¼Œä¸å†ä½¿ç”¨å›ºå®šå»¶è¿Ÿï¼‰
        tasks = [
            asyncio.create_task(process_type_with_semaphore(analysis_type))
            for analysis_type in analysis_types
        ]

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        await asyncio.gather(*tasks, return_exceptions=True)

        return {
            "success": True,
            "stock_code": stock_code,
            "successful_analyses": completed_tasks["successful"],
            "failed_analyses": completed_tasks["failed"],
            "total_analyses": len(analysis_types),
            "output_dir": str(stock_reports_dir)
        }

async def main():
    """ä¸»å¼‚æ­¥å‡½æ•°"""
    data_base_dir, output_dir, analysis_types, model_name = get_config()

    if len(sys.argv) < 2:
        print("âŒ ç”¨æ³•: python individual_stock_analyser.py <è‚¡ç¥¨ä»£ç >")
        return

    stock_code = sys.argv[1].strip()
    start_time = time.time()

    # æ„å»ºæ­£ç¡®çš„æ•°æ®ç›®å½•è·¯å¾„
    stock_data_dir = config.get_stock_dir(stock_code, cleaned=True)

    print(f"ğŸ¤– è‚¡ç¥¨AIåˆ†æ | {stock_code}")
    print(f"ğŸ“‚ æ•°æ®: {stock_data_dir}")
    print(f"ğŸ“ è¾“å‡º: {output_dir}")
    print("=" * 50)

    async with AsyncStockAIAnalyzer(model_name) as analyzer:
        result = await analyzer.process_stock_analysis(
            stock_code, analysis_types, str(stock_data_dir), str(output_dir)
        )
        
        total_time = time.time() - start_time
        print(f"\nğŸ“ˆ åˆ†æå®Œæˆ | æˆåŠŸç‡: {result['successful_analyses']}/{result['total_analyses']} | è€—æ—¶: {total_time:.2f}s")
        print(f"ğŸ“ æŠ¥å‘Šç›®å½•: {result['output_dir']}")

if __name__ == "__main__":
    run_main(main)